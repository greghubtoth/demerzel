{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from peft import LoraConfig, PeftConfig, get_peft_model\n",
    "from tqdm import tqdm\n",
    "from transformers import (AutoModelForCausalLM, AutoModelForSeq2SeqLM,\n",
    "                          AutoTokenizer, Trainer, TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nyx.evaluation import quantitative_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install peft -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[24]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 1\n",
    "EVALUATION_BATCH_SIZE = 4\n",
    "LEARNING_RATE = 1.41e-11  # 1e-3\n",
    "LORA_PARAM_R = 16\n",
    "LORA_PARAM_ALPHA = 32\n",
    "LORA_PARAM_TARGET_MODULES = {\n",
    "    \"bigscience/mt0-small\": [\"q\", \"v\"],\n",
    "    \"microsoft/phi-1_5\": [\"q_proj\", \"v_proj\"],\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\": [\"qkv_proj\"],\n",
    "    \"microsoft/Phi-3-medium-4k-instruct\": [\"qkv_proj\"],\n",
    "}\n",
    "# PRECISION = torch.float32\n",
    "PRECISION_NAME = 'bfloat16'\n",
    "DEVICE = \"cuda\"  # 0 if torch.cuda.is_available() else \"cpu\"\n",
    "CHOSEN_MODEL = \"microsoft/Phi-3-mini-4k-instruct\"  # \"microsoft/phi-1_5\"  \"bigscience/mt0-small\" \"google/flan-t5-large\"\n",
    "TESTING = True\n",
    "RUN_ID = uuid.uuid4().hex\n",
    "print(RUN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model: {CHOSEN_MODEL} will be trained on device: {DEVICE}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developed utility functions<br>\n",
    "- For details see [Bath github link](https://github.bath.ac.uk/gt566/ai-msc-dissertation/blob/dissertation-experienced-ft/nyx/dissertation/utils.py)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path.cwd().parent.absolute()\n",
    "nyx_path = f'{path}/'\n",
    "print(nyx_path)\n",
    "sys.path.append(nyx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nyx.constants import (COMMON_OUTPUT_PATHS, METRICS_PATH,\n",
    "                           SFT_DATA_OUTPUT_PATH, SFT_OUTPUT_DIR,\n",
    "                           SFT_PEFT_ADAPTER_PATH, SFT_PEFT_MERGED_MODEL_PATH)\n",
    "from nyx.utils import (download_and_save_reddit_data, get_task_type,\n",
    "                       precision_enumerator,\n",
    "                       print_number_of_trainable_model_parameters,\n",
    "                       round_dictionary_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMON_OUTPUT_PATHS = COMMON_OUTPUT_PATHS.format(RUN_ID=RUN_ID)\n",
    "METRICS_PATH = METRICS_PATH.format(COMMON_OUTPUT_PATHS=COMMON_OUTPUT_PATHS)\n",
    "COMMON_OUTPUT_PATHS = COMMON_OUTPUT_PATHS.format(RUN_ID=RUN_ID)\n",
    "SFT_OUTPUT_DIR = SFT_OUTPUT_DIR.format(COMMON_OUTPUT_PATHS=COMMON_OUTPUT_PATHS)\n",
    "SFT_PEFT_ADAPTER_PATH = SFT_PEFT_ADAPTER_PATH.format(\n",
    "    COMMON_OUTPUT_PATHS=COMMON_OUTPUT_PATHS\n",
    ")\n",
    "SFT_PEFT_MERGED_MODEL_PATH = SFT_PEFT_MERGED_MODEL_PATH.format(\n",
    "    COMMON_OUTPUT_PATHS=COMMON_OUTPUT_PATHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRECISION = precision_enumerator(PRECISION_NAME)\n",
    "PRECISION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[26]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    original_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        CHOSEN_MODEL,\n",
    "        torch_dtype=PRECISION,\n",
    "        device_map=\"auto\",  #  attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "except ValueError:\n",
    "    original_model = AutoModelForCausalLM.from_pretrained(\n",
    "        CHOSEN_MODEL,\n",
    "        torch_dtype=PRECISION,\n",
    "        device_map=\"auto\",  #  attn_implementation=\"flash_attention_2\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CHOSEN_MODEL, padding_side=\"left\"\n",
    ")  # model_max_length=512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original_model.to(torch.device(DEVICE))<br>\n",
    "print(\"Push models and data to GPU for efficiency.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[27]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_reddit_summarisation_data = Path(SFT_DATA_OUTPUT_PATH)\n",
    "if not filtered_reddit_summarisation_data.is_dir():\n",
    "    print(\"Downloading and saving filtered reddit data.\")\n",
    "    download_and_save_reddit_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(SFT_DATA_OUTPUT_PATH)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[28]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TESTING is True:\n",
    "    dataset[\"train\"] = dataset[\"train\"].select(range(200))\n",
    "    dataset[\"test\"] = dataset[\"test\"].select(range(100))\n",
    "    dataset[\"validation\"] = dataset[\"validation\"].select(range(50))\n",
    "    # dataset = dataset.filter(\n",
    "    #     lambda example, index: index % 4680 == 0, with_indices=True\n",
    "    # )\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[38]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = (\n",
    "    tokenizer.pad_token if tokenizer.pad_token is not None else tokenizer.eos_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = \"Summarize the following reddit post.\\n\\n\"\n",
    "    end_prompt = \"\\n\\nSummary: \"\n",
    "    prompt = [start_prompt + post + end_prompt for post in example[\"post\"]]\n",
    "    example['check'] = prompt\n",
    "    example[\"input_ids\"] = tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",  # padding=True\n",
    "    ).input_ids  # .to(torch.device(DEVICE))\n",
    "    example[\"labels\"] = tokenizer(\n",
    "        example[\"summary\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",  # padding=True\n",
    "    ).input_ids  # .to(torch.device(DEVICE))\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset actually contains 3 diff splits: train, validation, test.<br>\n",
    "The tokenize_function code is handling all data across all splits in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "    [\"id\", \"subreddit\", \"post\", \"summary\",]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[39]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train PEFT adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[31]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for layers to apply LoRA. Selecting the query and value layers are the most<br>\n",
    "basic implementation according to the paper. They are refered to as q and v here.<br>\n",
    "print(original_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[34]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    # Determines the size of LoRA matrices. x*r * r*y = x*y\n",
    "    r=LORA_PARAM_R,\n",
    "    # scaling coefficient. Paper mentions it is important because the adjustments are small compared\n",
    "    # to the rest of the model.\n",
    "    lora_alpha=LORA_PARAM_ALPHA,\n",
    "    # Variable target_modules determines what layers are fine-tuned, see architecture above.\n",
    "    # Simplest case scenario based on the original paper.\n",
    "    target_modules=LORA_PARAM_TARGET_MODULES[CHOSEN_MODEL],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=get_task_type(model=original_model),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[35]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(original_model, lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[40]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "common_folder_path = f\"./models/openai-subreddit-data/{CHOSEN_MODEL}/supervised-fine-tuning\"<br>\n",
    "output_dir = (<br>\n",
    "    f\"{common_folder_path}/peft-dialogue-summary-training-{str(int(time.time()))}\"<br>\n",
    ")<br>\n",
    "peft_model_path = f\"{common_folder_path}/peft-dialogue-summary-checkpoint-local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=SFT_OUTPUT_DIR,\n",
    "    # auto_find_batch_size=True,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,  # Higher learning rate than full fine-tuning.\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients over 2 steps\n",
    "    fp16=True,  # Enable mixed precision\n",
    "    # num_train_epochs=3,\n",
    "    save_steps=5_000,\n",
    "    logging_steps=1,\n",
    "    max_steps=len(tokenized_datasets[\"train\"])\n",
    "    // TRAIN_BATCH_SIZE,  # number of training data * 2, i.e. go over all data-summary pairs twice.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_trainer = Trainer(\n",
    "    model=peft_model,  # Important to train on Mac Chip GPU equivalent # .to(\n",
    "    #     torch.device(DEVICE)\n",
    "    # )\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[41]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_trainer.train()\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = end - start\n",
    "# print(end)\n",
    "print(f\"Training for 1 epoch took {round(duration, 2)} seconds to execute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_trainer.model.save_pretrained(SFT_PEFT_ADAPTER_PATH)\n",
    "print(f'Peft adapter was saved to:\\n{SFT_PEFT_ADAPTER_PATH}')\n",
    "tokenizer.save_pretrained(SFT_PEFT_ADAPTER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Load PEFT adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[15]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needed to add adapter_config.json to folder and change name of the model from pytorch_model.bin to adapter_model.bin<br>\n",
    "adapter_checkpoint_path = f\"/Users/gtoth/PycharmProjects/LLM-jupyter-notebooks/openai-subreddit-data-flan-t5-large/peft-dialogue-summary-checkpoint-local-6k\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adapter_checkpoint_path = f\"{common_folder_path}/peft-dialogue-summary-checkpoint-local\"<br>\n",
    "trained_model = AutoModelForSeq2SeqLM.from_pretrained(<br>\n",
    "    CHOSEN_MODEL, torch_dtype=PRECISION<br>\n",
    ")<br>\n",
    "try:<br>\n",
    "    trained_model = AutoModelForSeq2SeqLM.from_pretrained(<br>\n",
    "        CHOSEN_MODEL, torch_dtype=PRECISION<br>\n",
    "    )<br>\n",
    "except ValueError:<br>\n",
    "    trained_model = AutoModelForCausalLM.from_pretrained(<br>\n",
    "        CHOSEN_MODEL, torch_dtype=PRECISION<br>\n",
    "    )<br>\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Comparing PEFT and Baseline model generations (with ROUGE)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[18]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%timeit quantitative_comparison(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[20]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EVAL_SAMPLES = int(len(tokenized_datasets['test']) * 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_generation = quantitative_comparison(\n",
    "    original_model,\n",
    "    dataset,\n",
    "    tokenizer,\n",
    "    n_samples_to_evaluate=N_EVAL_SAMPLES,\n",
    "    batch_size=EVALUATION_BATCH_SIZE,\n",
    "    device=DEVICE,\n",
    ")\n",
    "# peft_checkpoint_model = PeftModel.from_pretrained(original_model, SFT_PEFT_ADAPTER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = PeftConfig.from_pretrained(SFT_PEFT_ADAPTER_PATH)\n",
    "# to initiate with random weights\n",
    "peft_config.init_lora_weights = False\n",
    "original_model.add_adapter(peft_config)\n",
    "original_model.enable_adapters()\n",
    "original_model  # .to(torch.device(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_checkpoint_generation = quantitative_comparison(\n",
    "    original_model,  # peft enabled model\n",
    "    dataset,\n",
    "    tokenizer,\n",
    "    n_samples_to_evaluate=N_EVAL_SAMPLES,\n",
    "    batch_size=EVALUATION_BATCH_SIZE,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = end - start\n",
    "print(\n",
    "    f\"Evaluating N={N_EVAL_SAMPLES} samples took {round(duration, 2)} seconds to execute.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_baseline_answer = dataset[\"test\"][0:N_EVAL_SAMPLES][\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_summaries = list(\n",
    "    zip(human_baseline_answer, peft_checkpoint_generation, baseline_model_generation)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    zipped_summaries,\n",
    "    columns=[\n",
    "        \"human_baseline_answer\",\n",
    "        \"peft_checkpoint_generation\",\n",
    "        \"baseline_model_generation\",\n",
    "    ],\n",
    ")\n",
    "df.head()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[21]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions=baseline_model_generation,\n",
    "    references=human_baseline_answer[0 : len(baseline_model_generation)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_checkpoint_generation,\n",
    "    references=human_baseline_answer[0 : len(peft_checkpoint_generation)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model_results = round_dictionary_values(original_model_results)\n",
    "# instruct_model_results = round_dictionary_values(instruct_model_results)\n",
    "peft_model_results = round_dictionary_values(peft_model_results)\n",
    "print(\"ORIGINAL MODEL:\")\n",
    "print(original_model_results)\n",
    "# print('INSTRUCT MODEL:')\n",
    "# print(instruct_model_results)\n",
    "print(\"PEFT MODEL:\")\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[27]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[29]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(METRICS_PATH):\n",
    "    os.makedirs(METRICS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f'{METRICS_PATH}/sft-results.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {\n",
    "    'rouge-metric-baseline-model': original_model_results,\n",
    "    'rouge-metric-sft-model': peft_model_results,\n",
    "    'train_batch_size': TRAIN_BATCH_SIZE,\n",
    "    'evaluation_batch_size': EVALUATION_BATCH_SIZE,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'lora_param_r': LORA_PARAM_R,\n",
    "    'lora_param_alpha': LORA_PARAM_ALPHA,\n",
    "    'lora_param_target_modules': LORA_PARAM_TARGET_MODULES[CHOSEN_MODEL],\n",
    "    'precision': PRECISION_NAME,\n",
    "    'device': DEVICE,\n",
    "    'chosen_model': CHOSEN_MODEL,\n",
    "    'testing': TESTING,\n",
    "    'run_id': RUN_ID,\n",
    "    'gpu_type': torch.cuda.get_device_name(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path, 'w') as file:\n",
    "    json.dump(results_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[22]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improvement = np.array(list(peft_model_results.values())) - np.array(\n",
    "    list(original_model_results.values())\n",
    ")\n",
    "for key, value in zip(peft_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge and save peft model (with base model)<br>\n",
    "So that, it can be loaded in as a Reward Moldel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[30]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.save_pretrained(SFT_PEFT_MERGED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
