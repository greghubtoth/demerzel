{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d7ae29-e41c-42ad-a5ce-ded3227265e9",
   "metadata": {},
   "source": [
    "#Â RLHF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c07a6ad-f8ec-4eac-94e4-49b0ff69eca0",
   "metadata": {},
   "source": [
    "## Reward Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd4eea259fe14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install peft trl weaviate-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcc63bef-276f-4c17-bb08-7ef664137a12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import torch\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from trl import RewardConfig, RewardTrainer\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead\n",
    "from trl import create_reference_model\n",
    "from trl.core import LengthSampler\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "TESTING = False\n",
    "\n",
    "PRECISION_NAME = 'float16'\n",
    "DEVICE = \"cuda\"\n",
    "CHOSEN_MODEL = \"microsoft/phi-1_5\"\n",
    "# \"microsoft/phi-1_5\" #\"bigscience/mt0-small\" # \"google/flan-t5-large\" \"stabilityai/stablelm-2-zephyr-1_6b\"\n",
    "RANDOM_SEED = 42\n",
    "RUN_ID = \"db816086ae284b9ea8cb56fc799897a8\" # SFT MODEL # uuid.uuid4().hex  \n",
    "\n",
    "LORA_PARAM_TARGET_MODULES = {\n",
    "    \"bigscience/mt0-small\": [\"q\", \"v\"],\n",
    "    \"microsoft/phi-1_5\": [\"q_proj\", \"v_proj\"],\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\": [\"qkv_proj\"],\n",
    "    \"microsoft/Phi-3-medium-4k-instruct\": [\"qkv_proj\"],\n",
    "}\n",
    "\n",
    "RM_LORA_PARAM_R = 32\n",
    "RM_LORA_PARAM_ALPHA = 64\n",
    "RM_LORA_PARAM_TARGET_MODULES = LORA_PARAM_TARGET_MODULES[CHOSEN_MODEL] + [\n",
    "    \"dense\",\n",
    "    \"out_proj\",\n",
    "]\n",
    "RM_TRAIN_BATCH_SIZE = 5\n",
    "RM_LEARNING_RATE = 1e-5\n",
    "RM_TRAIN_DATA_RUN_ID = \"b592e2304b5c4b41a33236871ed4e195\"\n",
    "\n",
    "RL_LORA_PARAM_R = 32\n",
    "RL_LORA_PARAM_ALPHA = 64\n",
    "RL_LORA_PARAM_TARGET_MODULES = LORA_PARAM_TARGET_MODULES[CHOSEN_MODEL]\n",
    "RL_TRAIN_BATCH_SIZE = 2\n",
    "RL_TRAIN_MINI_BATCH_SIZE = 1\n",
    "RL_LEARNING_RATE = 1.41e-7\n",
    "RL_N_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35001d4a8ca68551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/demerzel/\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path.cwd().parent.absolute()\n",
    "nyx_path = f'{path}/'\n",
    "print(nyx_path)\n",
    "sys.path.append(nyx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "104a2bd6-d01c-42d1-ae6e-cf57d485c161",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nyx.evaluation import quantitative_comparison\n",
    "from nyx.data_generation.evaluators import AILabelEvaluator\n",
    "\n",
    "from nyx.utils import (\n",
    "    precision_enumerator,\n",
    "    print_number_of_trainable_model_parameters,\n",
    "    get_task_type,\n",
    ")\n",
    "from nyx.constants import (\n",
    "    COMPARISON_DATA_PATH,\n",
    "    SFT_DATA_OUTPUT_PATH,\n",
    "    COMMON_OUTPUT_PATHS,\n",
    "    SFT_PEFT_MERGED_MODEL_PATH,\n",
    "    SFT_PEFT_ADAPTER_PATH,\n",
    "    RM_TRAIN_DATA_PATH,\n",
    "    RM_OUTPUT_DIR,\n",
    "    RM_PEFT_ADAPTER_PATH,\n",
    "    RM_PEFT_MERGED_MODEL_PATH,\n",
    "    METRICS_PATH,\n",
    ")\n",
    "\n",
    "\n",
    "common_output_path = COMMON_OUTPUT_PATHS.format(RUN_ID=RUN_ID)\n",
    "\n",
    "SFT_PEFT_ADAPTER_PATH = SFT_PEFT_ADAPTER_PATH.format(\n",
    "    COMMON_OUTPUT_PATHS=common_output_path\n",
    ")\n",
    "SFT_PEFT_MERGED_MODEL_PATH = SFT_PEFT_MERGED_MODEL_PATH.format(\n",
    "    COMMON_OUTPUT_PATHS=common_output_path\n",
    ")\n",
    "\n",
    "RM_OUTPUT_DIR = RM_OUTPUT_DIR.format(COMMON_OUTPUT_PATHS=common_output_path)\n",
    "RM_PEFT_ADAPTER_PATH = RM_PEFT_ADAPTER_PATH.format(\n",
    "    COMMON_OUTPUT_PATHS=common_output_path\n",
    ")\n",
    "RM_PEFT_MERGED_MODEL_PATH = RM_PEFT_MERGED_MODEL_PATH.format(\n",
    "    COMMON_OUTPUT_PATHS=common_output_path\n",
    ")\n",
    "\n",
    "# if generated data from a different run needs to be utilised.\n",
    "if RM_TRAIN_DATA_RUN_ID is not None:\n",
    "    rm_common_path = COMMON_OUTPUT_PATHS.format(RUN_ID=RM_TRAIN_DATA_RUN_ID)\n",
    "    RM_TRAIN_DATA_PATH = RM_TRAIN_DATA_PATH.format(COMMON_OUTPUT_PATHS=rm_common_path)\n",
    "else:  # utilise current run_id\n",
    "    RM_TRAIN_DATA_PATH = RM_TRAIN_DATA_PATH.format(\n",
    "        COMMON_OUTPUT_PATHS=common_output_path\n",
    "    )\n",
    "\n",
    "METRICS_PATH = METRICS_PATH.format(COMMON_OUTPUT_PATHS=common_output_path)\n",
    "\n",
    "PRECISION = precision_enumerator(PRECISION_NAME)\n",
    "PRECISION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc63b85-1e21-42d8-873f-7eab599bb773",
   "metadata": {},
   "source": [
    "### Load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55837b16-932f-4a7e-b2c0-4996fc17c991",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['subreddit', 'post', 'choice', 'candidate_summary_1', 'candidate_summary_2'],\n",
       "        num_rows: 92858\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['subreddit', 'post', 'choice', 'candidate_summary_1', 'candidate_summary_2'],\n",
       "        num_rows: 83802\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original dataset with train, test and validation\n",
    "comparison_dataset = load_from_disk(COMPARISON_DATA_PATH)\n",
    "\n",
    "# Train dataset with generated AI labels\n",
    "comparison_train_dataset = load_from_disk(RM_TRAIN_DATA_PATH)\n",
    "\n",
    "if TESTING is True:\n",
    "    comparison_dataset = comparison_dataset.filter(\n",
    "        lambda example, index: index % 10 == 0, with_indices=True\n",
    "    )\n",
    "comparison_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26897fad-3569-4c97-b272-7974f4cdf771",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    sft_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        SFT_PEFT_MERGED_MODEL_PATH,\n",
    "        torch_dtype=PRECISION,\n",
    "        device_map=\"auto\",\n",
    "        # attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "except ValueError:\n",
    "    sft_model = AutoModelForCausalLM.from_pretrained(\n",
    "        SFT_PEFT_MERGED_MODEL_PATH,\n",
    "        torch_dtype=PRECISION,\n",
    "        device_map=\"auto\",\n",
    "#         attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHOSEN_MODEL, padding_side='left')\n",
    "tokenizer.pad_token = (\n",
    "    tokenizer.pad_token if tokenizer.pad_token is not None else tokenizer.eos_token  # '<|endoftext|>'\n",
    ")\n",
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf572f1e-abf9-4361-b1a4-ae1e53b3fd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_model.config.pad_token_id = sft_model.config.eos_token_id\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb6405eb-8647-4a90-8213-7ffdd6d8f87b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_summary_cols(example):\n",
    "    start_prompt = \"Summarize the following reddit post.\\n\\n\"\n",
    "    end_prompt = \"\\n\\nSummary: \"\n",
    "    example['summary_prompts_1'] = (\n",
    "        start_prompt + example[\"post\"] + end_prompt + example[\"candidate_summary_1\"]\n",
    "    )\n",
    "    example['summary_prompts_2'] = (\n",
    "        start_prompt + example[\"post\"] + end_prompt + example[\"candidate_summary_2\"]\n",
    "    )\n",
    "    return example\n",
    "\n",
    "\n",
    "comparison_train_dataset = comparison_train_dataset.map(create_summary_cols)\n",
    "\n",
    "# tokenized_train_dataset['train']['summary_prompts_1'][0]\n",
    "\n",
    "\n",
    "def prepare_for_reward_modelling(example, hf_baseline: bool = False):\n",
    "    choice_column = example[\"choice\"] if hf_baseline is True else example[\"ai_choice\"]\n",
    "    # ai_choice is based on index choice 0 ==summary 1, choice 1 == summary 2\n",
    "    example[\"accepted_summary\"] = (\n",
    "        example['summary_prompts_2']\n",
    "        if choice_column == example[\"constant_col\"]\n",
    "        else example['summary_prompts_1']\n",
    "    )\n",
    "    example[\"rejected_summary\"] = (\n",
    "        example['summary_prompts_1']\n",
    "        if choice_column == example[\"constant_col\"]\n",
    "        else example['summary_prompts_2']\n",
    "    )\n",
    "    return example\n",
    "\n",
    "\n",
    "comparison_train_dataset = comparison_train_dataset.map(prepare_for_reward_modelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4680a11f-6ba4-4f7b-9088-b2dee2611d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['subreddit', 'post', 'choice', 'candidate_summary_1', 'candidate_summary_2', 'ai_choice', 'ai_choice_for_prompt', 'constant_col', 'nth_retry', 'ordered_prompt_used_to_predict', 'reversed_prompt_used_to_predict', 'incorrect_prediction', 'summary_prompts_1', 'summary_prompts_2', 'accepted_summary', 'rejected_summary'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf38cb41-b8c7-4c34-a29e-db3019f7b728",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "Encoder-Decoder specific line 29<br>\n",
    "Sampling from a range of start and end prompts could help robustness in the RM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10614346-71ce-4aee-be5b-4a7cf693c573",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b88fb7f98b459c8527af255b0cea01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ai_choice_for_prompt', 'nth_retry', 'ordered_prompt_used_to_predict', 'reversed_prompt_used_to_predict', 'incorrect_prediction', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    # start_prompt = \"Summarize the following reddit post.\\n\\n\"\n",
    "    # end_prompt = \"\\n\\nSummary: \"\n",
    "    # prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"post\"]]\n",
    "    accepted = tokenizer(\n",
    "        example[\"accepted_summary\"],\n",
    "        padding=True,\n",
    "        # padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(torch.device(DEVICE))\n",
    "    example[\"input_ids_chosen\"] = accepted.input_ids\n",
    "    example[\"attention_mask_chosen\"] = accepted.attention_mask\n",
    "    rejected = tokenizer(\n",
    "        example[\"rejected_summary\"],\n",
    "        padding=True,\n",
    "        # padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(torch.device(DEVICE))\n",
    "    example[\"input_ids_rejected\"] = rejected.input_ids\n",
    "    example[\"attention_mask_rejected\"] = rejected.attention_mask\n",
    "\n",
    "    example[\"labels\"] = tokenizer(\n",
    "        [str(choice) for choice in example[\"ai_choice\"]],\n",
    "        padding=True,\n",
    "        # padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).input_ids.to(torch.device(DEVICE))\n",
    "\n",
    "    if 'causal' in sft_model.config.architectures[0].lower() and sft_model.config.architectures[0] != 'PhiForCausalLM':\n",
    "        example[\"decoder_input_ids\"] = sft_model._shift_right(example[\"labels\"])\n",
    "    return example\n",
    "\n",
    "\n",
    "reward_modelling_train_dataset = comparison_train_dataset.map(\n",
    "    tokenize_function, batched=True\n",
    ")\n",
    "reward_modelling_train_dataset = reward_modelling_train_dataset.remove_columns(\n",
    "    [\n",
    "        'subreddit',\n",
    "        'post',\n",
    "        'choice',\n",
    "        'candidate_summary_1',\n",
    "        'candidate_summary_2',\n",
    "        # 'prompts',\n",
    "        'ai_choice',\n",
    "        'constant_col',\n",
    "        'accepted_summary',\n",
    "        'rejected_summary',\n",
    "        'summary_prompts_2',\n",
    "        'summary_prompts_1',\n",
    "    ]  # 'is_match',\n",
    ")\n",
    "reward_modelling_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5a0abd-4616-4465-942b-677aad7487b5",
   "metadata": {},
   "source": [
    "The below model is the pretrained SFT model with an additional head for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afbbbf3b-53e2-41d5-88c5-736aa3dff749",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PhiForSequenceClassification were not initialized from the model checkpoint at microsoft/phi-1_5 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 9441280\n",
      "all model parameters: 1322807296\n",
      "percentage of trainable model parameters: 0.71%\n"
     ]
    }
   ],
   "source": [
    "base_reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    SFT_PEFT_MERGED_MODEL_PATH, torch_dtype=PRECISION\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    # Determines the size of LoRA matrices. x*r * r*y = x*y\n",
    "    r=RM_LORA_PARAM_R,\n",
    "    # scaling coefficient. Paper mentions it is important because the adjustments are small compared\n",
    "    # to the rest of the model.\n",
    "    lora_alpha=RM_LORA_PARAM_ALPHA,\n",
    "    # Variable target_modules determines what layers are fine-tuned, see architecture above.\n",
    "    # Simplest case scenario based on the original paper.\n",
    "    # The parameters / layers of the new head need to be enabled for training.\n",
    "    target_modules=RM_LORA_PARAM_TARGET_MODULES,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    ")\n",
    "\n",
    "rm_peft_model = get_peft_model(base_reward_model, lora_config)\n",
    "print(print_number_of_trainable_model_parameters(rm_peft_model))\n",
    "# base_reward_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea0b1db0-0070-46c2-a48a-0f5c6b0ec118",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reward_modelling_train_dataset['train']\n",
    "# print(tokenizer.eos_token_id)\n",
    "base_reward_model.config.pad_token_id = tokenizer.eos_token_id\n",
    "base_reward_model.config.pad_token = tokenizer.pad_token\n",
    "# base_reward_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46e025ce-3743-4588-8b0d-0794610a9dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/reward_trainer.py:175: UserWarning: When using RewardDataCollatorWithPadding, you should set `max_length` in RewardConfig. It will be set to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/reward_trainer.py:192: UserWarning: When using RewardDataCollatorWithPadding, you should set `remove_unused_columns=False` in your RewardConfig we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "training_args = RewardConfig(\n",
    "    output_dir=RM_OUTPUT_DIR,\n",
    "    # auto_find_batch_size=True,\n",
    "    per_device_train_batch_size=RM_TRAIN_BATCH_SIZE,\n",
    "    save_steps=500,\n",
    "    learning_rate=RM_LEARNING_RATE,  # Higher learning rate than full fine-tuning.\n",
    "    logging_steps=1,\n",
    "    max_steps=len(reward_modelling_train_dataset['train']) // RM_TRAIN_BATCH_SIZE,\n",
    ")\n",
    "\n",
    "reward_trainer = RewardTrainer(  # trainer class child\n",
    "    model=rm_peft_model,\n",
    "    args=training_args,  # trainerarguments child\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=reward_modelling_train_dataset['train'],\n",
    "    # num_labels=1, # Regression\n",
    ")\n",
    "\n",
    "# for _, param in peft_model.named_parameters():\n",
    "#         # all_model_params += param.numel()\n",
    "#         param.requires_grad = True\n",
    "# print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2675f352-0772-4c39-921c-d18f123853dd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CodeGenTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='340' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 340/2000 15:48 < 1:17:36, 0.36 it/s, Epoch 0.34/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.681200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>309</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>311</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>313</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>329</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>338</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "reward_trainer.train()\n",
    "end = time.time()\n",
    "\n",
    "duration = end - start\n",
    "print(end)\n",
    "print(f\"Training for 1 epoch took {round(duration, 2)} seconds to execute.\")\n",
    "\n",
    "reward_trainer.model.save_pretrained(RM_PEFT_ADAPTER_PATH)\n",
    "tokenizer.save_pretrained(RM_PEFT_ADAPTER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0958ff-a546-4a45-b658-c5a047c079d5",
   "metadata": {},
   "source": [
    "### Reload and evaluate RM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8e2b55-4b46-4dc6-9336-9f9c07592a08",
   "metadata": {},
   "source": [
    "### Merge and save RM model (with base model)\n",
    "So that, the sentiment pipe warning is eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da40dc71-09f9-485c-9aad-a289960ba6d7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging and saving the model which is trained all the way (i.e., utilising all of the data).\n",
    "merged_rm_model = rm_peft_model.merge_and_unload()\n",
    "merged_rm_model.save_pretrained(RM_PEFT_MERGED_MODEL_PATH)\n",
    "\n",
    "merged_rm_model.to(torch.device(DEVICE))\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9db1ed0-72cc-40f4-8739-0de204a2c166",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Loop this over [base, 500, 1000, 5000, 10_000, 50_000, 100_000] to calculate the pairwise accuracy\n",
    "\n",
    "\n",
    "# peft_rm_model = PeftModel.from_pretrained(base_reward_model,\n",
    "#                                        f'{rm_peft_model_path}',\n",
    "#                                        torch_dtype=PRECISION,\n",
    "#                                        is_trainable=False)\n",
    "\n",
    "# peft_rm_model.to(torch.device('mps'))\n",
    "# print('PEFT trained RM is loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5e71a9-55a7-473c-8bd4-2470cbfc68b2",
   "metadata": {},
   "source": [
    "### _Comparing HF and AIF data (RM model generated)_\n",
    "Utilising the train test split function to select a random 15% of the validation set to compare HF and AIF labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25280a3c-3f3d-4386-853f-ef811f5f022a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "rm_eval_data = comparison_dataset['validation'].train_test_split(\n",
    "    test_size=0.15, seed=RANDOM_SEED\n",
    ")\n",
    "rm_eval_data['test']\n",
    "print(len(rm_eval_data['test']))\n",
    "\n",
    "\n",
    "def get_rm_probabilities(\n",
    "    col: str, rm_model_to_evaluate=merged_rm_model\n",
    ") -> List[List[str]]:\n",
    "    candidate = rm_model_to_evaluate(\n",
    "        tokenizer(\n",
    "            rm_eval_data['test'][col],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids.to(DEVICE)\n",
    "    )\n",
    "\n",
    "    candidate_probabilities = candidate.logits.softmax(dim=-1).tolist()\n",
    "    return candidate_probabilities\n",
    "\n",
    "\n",
    "candidate_1_probabilities = get_rm_probabilities('candidate_summary_1')\n",
    "candidate_2_probabilities = get_rm_probabilities('candidate_summary_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcc60384-48ea-4608-a4c6-c412fe1c8a59",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a69d73a1c4471d9aea038008cba27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Reward Model (RM) is in agreement with the annotator provided labels: 0.0% of the times.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['subreddit', 'post', 'choice', 'candidate_summary_1', 'candidate_summary_2'],\n",
       "        num_rows: 15\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['subreddit', 'post', 'choice', 'candidate_summary_1', 'candidate_summary_2', 'candidate_1_preference_probability', 'candidate_2_preference_probability', 'rm_choice', 'is_match'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking the first item in probabilities will yield the probability of being a chosen summary.\n",
    "rm_eval_data['test'] = rm_eval_data['test'].add_column(\n",
    "    name=\"candidate_1_preference_probability\",\n",
    "    column=[item[0] for item in candidate_1_probabilities],\n",
    ")\n",
    "rm_eval_data['test'] = rm_eval_data['test'].add_column(\n",
    "    name=\"candidate_2_preference_probability\",\n",
    "    column=[item[0] for item in candidate_2_probabilities],\n",
    ")\n",
    "\n",
    "\n",
    "def get_rm_labels(example):\n",
    "    example[\"rm_choice\"] = (\n",
    "        0\n",
    "        if example[\"candidate_1_preference_probability\"]\n",
    "        >= example[\"candidate_2_preference_probability\"]\n",
    "        else 1\n",
    "    )\n",
    "    example[\"is_match\"] = 1 if example[\"choice\"] == example[\"rm_choice\"] else 0\n",
    "    return example\n",
    "\n",
    "\n",
    "# Apply the function to each example in the dataset\n",
    "rm_eval_data['test'] = rm_eval_data['test'].map(get_rm_labels)\n",
    "\n",
    "# Calculate the mean value of the 'is_match' feature\n",
    "rm_aggreement_mean_value = np.round(np.mean(rm_eval_data['test'][\"is_match\"]) * 100, 2)\n",
    "print(\n",
    "    f\"The Reward Model (RM) is in agreement with the annotator provided labels: {rm_aggreement_mean_value}% of the times.\"\n",
    ")\n",
    "rm_eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "768397aa-c869-405c-8459-7a2ebc7cdda5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c422653ed54609bcda2749d306649a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt driven, instruct model generated feedback labels are in agreement with the annotator provided labels: 0.0% of the times.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       0.0\n",
      "           1       0.00      0.00      0.00       3.0\n",
      "\n",
      "    accuracy                           0.00       3.0\n",
      "   macro avg       0.00      0.00      0.00       3.0\n",
      "weighted avg       0.00      0.00      0.00       3.0\n",
      "\n",
      "tp: 0, fp: 0\n",
      "fn: 3, tn: 0\n",
      "MCC is 0.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owner/opt/anaconda3/envs/llm-diss/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owner/opt/anaconda3/envs/llm-diss/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owner/opt/anaconda3/envs/llm-diss/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owner/opt/anaconda3/envs/llm-diss/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owner/opt/anaconda3/envs/llm-diss/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owner/opt/anaconda3/envs/llm-diss/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owner/opt/anaconda3/envs/llm-diss/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owner/opt/anaconda3/envs/llm-diss/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owner/opt/anaconda3/envs/llm-diss/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owner/opt/anaconda3/envs/llm-diss/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owner/opt/anaconda3/envs/llm-diss/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/owner/opt/anaconda3/envs/llm-diss/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "evaluator = AILabelEvaluator(data_to_evaluate=rm_eval_data, run_id=RUN_ID)\n",
    "\n",
    "evaluator.compute_metrics(data_split='test', predicted_col='rm_choice')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd9c288-ad4f-49af-9915-417a9e1ef57f",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc4231f3-4225-421f-8f08-8985d2ec6b40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'subreddit', 'title', 'post', 'summary'],\n",
       "        num_rows: 116722\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'subreddit', 'title', 'post', 'summary'],\n",
       "        num_rows: 6553\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'subreddit', 'title', 'post', 'summary'],\n",
       "        num_rows: 6447\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_disk(SFT_DATA_OUTPUT_PATH)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1719748-0188-4a0a-badb-71f0ca54df36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'subreddit', 'title', 'post', 'summary'],\n",
       "        num_rows: 25\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'subreddit', 'title', 'post', 'summary'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'subreddit', 'title', 'post', 'summary'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if TESTING is True:\n",
    "    dataset = dataset.filter(\n",
    "        lambda example, index: index % 4680 == 0, with_indices=True\n",
    "    )\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0249e3a2-1574-4afd-ae97-a61c042964a8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 688128\n",
      "all model parameters: 300864896\n",
      "percentage of trainable model parameters: 0.23%\n"
     ]
    }
   ],
   "source": [
    "rl_lora_config = LoraConfig(\n",
    "    r=RL_LORA_PARAM_R,  # Rank\n",
    "    lora_alpha=RL_LORA_PARAM_ALPHA,\n",
    "    target_modules=RL_LORA_PARAM_TARGET_MODULES,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=get_task_type(model=sft_model),\n",
    ")\n",
    "\n",
    "\n",
    "rl_peft_model = get_peft_model(sft_model, rl_lora_config)\n",
    "print(print_number_of_trainable_model_parameters(rl_peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8571a9c-4df0-45d8-8200-44976b8625b5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO model has trainable model parameters: 688641\n",
      "all model parameters: 300865409\n",
      "percentage of trainable model parameters: 0.23%\n",
      "\n",
      "AutoModelForSeq2SeqLMWithValueHead(\n",
      "  (pretrained_model): PeftModelForSeq2SeqLM(\n",
      "    (base_model): LoraModel(\n",
      "      (model): MT5ForConditionalGeneration(\n",
      "        (shared): Embedding(250112, 512)\n",
      "        (encoder): MT5Stack(\n",
      "          (embed_tokens): Embedding(250112, 512)\n",
      "          (block): ModuleList(\n",
      "            (0): MT5Block(\n",
      "              (layer): ModuleList(\n",
      "                (0): MT5LayerSelfAttention(\n",
      "                  (SelfAttention): MT5Attention(\n",
      "                    (q): lora.Linear(\n",
      "                      (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Dropout(p=0.05, inplace=False)\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=512, out_features=16, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=16, out_features=384, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                    )\n",
      "                    (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (v): lora.Linear(\n",
      "                      (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Dropout(p=0.05, inplace=False)\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=512, out_features=16, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=16, out_features=384, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                    )\n",
      "                    (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                    (relative_attention_bias): Embedding(32, 6)\n",
      "                  )\n",
      "                  (layer_norm): MT5LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (1): MT5LayerFF(\n",
      "                  (DenseReluDense): MT5DenseGatedActDense(\n",
      "                    (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                    (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                    (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (act): NewGELUActivation()\n",
      "                  )\n",
      "                  (layer_norm): MT5LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1-7): 7 x MT5Block(\n",
      "              (layer): ModuleList(\n",
      "                (0): MT5LayerSelfAttention(\n",
      "                  (SelfAttention): MT5Attention(\n",
      "                    (q): lora.Linear(\n",
      "                      (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Dropout(p=0.05, inplace=False)\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=512, out_features=16, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=16, out_features=384, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                    )\n",
      "                    (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (v): lora.Linear(\n",
      "                      (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Dropout(p=0.05, inplace=False)\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=512, out_features=16, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=16, out_features=384, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                    )\n",
      "                    (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                  )\n",
      "                  (layer_norm): MT5LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (1): MT5LayerFF(\n",
      "                  (DenseReluDense): MT5DenseGatedActDense(\n",
      "                    (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                    (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                    (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (act): NewGELUActivation()\n",
      "                  )\n",
      "                  (layer_norm): MT5LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MT5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (decoder): MT5Stack(\n",
      "          (embed_tokens): Embedding(250112, 512)\n",
      "          (block): ModuleList(\n",
      "            (0): MT5Block(\n",
      "              (layer): ModuleList(\n",
      "                (0): MT5LayerSelfAttention(\n",
      "                  (SelfAttention): MT5Attention(\n",
      "                    (q): lora.Linear(\n",
      "                      (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Dropout(p=0.05, inplace=False)\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=512, out_features=16, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=16, out_features=384, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                    )\n",
      "                    (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (v): lora.Linear(\n",
      "                      (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Dropout(p=0.05, inplace=False)\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=512, out_features=16, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=16, out_features=384, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                    )\n",
      "                    (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                    (relative_attention_bias): Embedding(32, 6)\n",
      "                  )\n",
      "                  (layer_norm): MT5LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (1): MT5LayerCrossAttention(\n",
      "                  (EncDecAttention): MT5Attention(\n",
      "                    (q): lora.Linear(\n",
      "                      (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Dropout(p=0.05, inplace=False)\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=512, out_features=16, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=16, out_features=384, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                    )\n",
      "                    (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (v): lora.Linear(\n",
      "                      (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Dropout(p=0.05, inplace=False)\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=512, out_features=16, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=16, out_features=384, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                    )\n",
      "                    (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                  )\n",
      "                  (layer_norm): MT5LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (2): MT5LayerFF(\n",
      "                  (DenseReluDense): MT5DenseGatedActDense(\n",
      "                    (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                    (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                    (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (act): NewGELUActivation()\n",
      "                  )\n",
      "                  (layer_norm): MT5LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1-7): 7 x MT5Block(\n",
      "              (layer): ModuleList(\n",
      "                (0): MT5LayerSelfAttention(\n",
      "                  (SelfAttention): MT5Attention(\n",
      "                    (q): lora.Linear(\n",
      "                      (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Dropout(p=0.05, inplace=False)\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=512, out_features=16, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=16, out_features=384, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                    )\n",
      "                    (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (v): lora.Linear(\n",
      "                      (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Dropout(p=0.05, inplace=False)\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=512, out_features=16, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=16, out_features=384, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                    )\n",
      "                    (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                  )\n",
      "                  (layer_norm): MT5LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (1): MT5LayerCrossAttention(\n",
      "                  (EncDecAttention): MT5Attention(\n",
      "                    (q): lora.Linear(\n",
      "                      (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Dropout(p=0.05, inplace=False)\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=512, out_features=16, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=16, out_features=384, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                    )\n",
      "                    (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (v): lora.Linear(\n",
      "                      (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Dropout(p=0.05, inplace=False)\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=512, out_features=16, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=16, out_features=384, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                    )\n",
      "                    (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                  )\n",
      "                  (layer_norm): MT5LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (2): MT5LayerFF(\n",
      "                  (DenseReluDense): MT5DenseGatedActDense(\n",
      "                    (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                    (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                    (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                    (act): NewGELUActivation()\n",
      "                  )\n",
      "                  (layer_norm): MT5LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (final_layer_norm): MT5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (lm_head): Linear(in_features=512, out_features=250112, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (v_head): ValueHead(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (summary): Linear(in_features=512, out_features=1, bias=True)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AutoModelForSeq2SeqLMWithValueHead(\n",
       "  (pretrained_model): PeftModelForSeq2SeqLM(\n",
       "    (base_model): LoraModel(\n",
       "      (model): MT5ForConditionalGeneration(\n",
       "        (shared): Embedding(250112, 512)\n",
       "        (encoder): MT5Stack(\n",
       "          (embed_tokens): Embedding(250112, 512)\n",
       "          (block): ModuleList(\n",
       "            (0): MT5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): MT5LayerSelfAttention(\n",
       "                  (SelfAttention): MT5Attention(\n",
       "                    (q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=512, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=384, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                    (v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=512, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=384, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "                    (relative_attention_bias): Embedding(32, 6)\n",
       "                  )\n",
       "                  (layer_norm): MT5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): MT5LayerFF(\n",
       "                  (DenseReluDense): MT5DenseGatedActDense(\n",
       "                    (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                    (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                    (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): NewGELUActivation()\n",
       "                  )\n",
       "                  (layer_norm): MT5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1-7): 7 x MT5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): MT5LayerSelfAttention(\n",
       "                  (SelfAttention): MT5Attention(\n",
       "                    (q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=512, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=384, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                    (v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=512, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=384, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): MT5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): MT5LayerFF(\n",
       "                  (DenseReluDense): MT5DenseGatedActDense(\n",
       "                    (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                    (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                    (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): NewGELUActivation()\n",
       "                  )\n",
       "                  (layer_norm): MT5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MT5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (decoder): MT5Stack(\n",
       "          (embed_tokens): Embedding(250112, 512)\n",
       "          (block): ModuleList(\n",
       "            (0): MT5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): MT5LayerSelfAttention(\n",
       "                  (SelfAttention): MT5Attention(\n",
       "                    (q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=512, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=384, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                    (v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=512, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=384, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "                    (relative_attention_bias): Embedding(32, 6)\n",
       "                  )\n",
       "                  (layer_norm): MT5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): MT5LayerCrossAttention(\n",
       "                  (EncDecAttention): MT5Attention(\n",
       "                    (q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=512, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=384, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                    (v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=512, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=384, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): MT5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (2): MT5LayerFF(\n",
       "                  (DenseReluDense): MT5DenseGatedActDense(\n",
       "                    (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                    (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                    (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): NewGELUActivation()\n",
       "                  )\n",
       "                  (layer_norm): MT5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1-7): 7 x MT5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): MT5LayerSelfAttention(\n",
       "                  (SelfAttention): MT5Attention(\n",
       "                    (q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=512, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=384, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                    (v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=512, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=384, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): MT5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): MT5LayerCrossAttention(\n",
       "                  (EncDecAttention): MT5Attention(\n",
       "                    (q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=512, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=384, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                    (v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=512, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=384, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): MT5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (2): MT5LayerFF(\n",
       "                  (DenseReluDense): MT5DenseGatedActDense(\n",
       "                    (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                    (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                    (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): NewGELUActivation()\n",
       "                  )\n",
       "                  (layer_norm): MT5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MT5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (lm_head): Linear(in_features=512, out_features=250112, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (v_head): ValueHead(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (summary): Linear(in_features=512, out_features=1, bias=True)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A transformer model with an additional scalar output for each token which can be used as a value function in reinforcement learning\n",
    "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(\n",
    "    rl_peft_model, torch_dtype=PRECISION, is_trainable=True\n",
    ")\n",
    "\n",
    "print(f'PPO model has {print_number_of_trainable_model_parameters(ppo_model)}\\n')\n",
    "print(ppo_model)\n",
    "ppo_model.to(torch.device(DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5f926a-88c4-47db-a9dd-5c742731ba31",
   "metadata": {},
   "source": [
    "The below function could also be adapted to sample from a variety of prompts to improve exploration and improve\n",
    "robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "159cfba9-435f-4afd-88a2-00a114fa956b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01bdcdedca3b412c817a7a2b3db72a2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'subreddit', 'title', 'post', 'summary', 'input_ids', 'query'],\n",
       "        num_rows: 25\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'subreddit', 'title', 'post', 'summary', 'input_ids', 'query'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'subreddit', 'title', 'post', 'summary', 'input_ids', 'query'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_for_rl(sample):\n",
    "    # Wrap each dialogue with the instruction.\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following reddit post.\n",
    "\n",
    "{sample[\"post\"]}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "    sample[\"input_ids\"] = tokenizer.encode(prompt)\n",
    "\n",
    "    # Requirement for PPO library.\n",
    "    sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "    return sample\n",
    "\n",
    "\n",
    "# Tokenize each dialogue.\n",
    "dataset = dataset.map(tokenize_for_rl, batched=False)\n",
    "dataset.set_format(type=\"torch\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ddc80b03-568d-4ea4-990c-8264e8142fc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference model parameters to be updated:\n",
      "trainable model parameters: 0\n",
      "all model parameters: 300865409\n",
      "percentage of trainable model parameters: 0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ref_model = create_reference_model(ppo_model)\n",
    "ref_model.to(torch.device(DEVICE))\n",
    "print(\n",
    "    f'Reference model parameters to be updated:\\n{print_number_of_trainable_model_parameters(ref_model)}\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c8a7ee22-9c33-40d1-af58-996b9e236ca0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collator input: [{'key1': 'value1', 'key2': 'value2', 'key3': 'value3'}, {'key1': 'value2', 'key2': 'value3', 'key3': 'value4'}]\n",
      "Collator output: {'key1': ['value1', 'value2'], 'key2': ['value2', 'value3'], 'key3': ['value3', 'value4']}\n"
     ]
    }
   ],
   "source": [
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "\n",
    "test_data = [\n",
    "    {\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"},\n",
    "    {\"key1\": \"value2\", \"key2\": \"value3\", \"key3\": \"value4\"},\n",
    "]\n",
    "print(f'Collator input: {test_data}')\n",
    "print(f'Collator output: {collator(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b6826a3d-2c88-4011-af76-6ec5be7bea6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "    # Name of model to use - used only for tracking purposes\n",
    "    model_name=CHOSEN_MODEL,\n",
    "    learning_rate=RL_LEARNING_RATE,\n",
    "    ppo_epochs=RL_N_EPOCHS,\n",
    "    mini_batch_size=RL_TRAIN_MINI_BATCH_SIZE,\n",
    "    batch_size=RL_TRAIN_BATCH_SIZE,\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=config,\n",
    "    model=ppo_model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset[\"train\"],\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "70529ba8-fe82-40bb-82e6-c0bf19c42369",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment_pipe = pipeline(\n",
    "    \"sentiment-analysis\", model=merged_rm_model, tokenizer=tokenizer, device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "039d4017-d2a2-453f-9e83-b4fd7640a55e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::isin.Tensor_Tensor_out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m     max_new_tokens \u001b[38;5;241m=\u001b[39m output_length_sampler()        \n\u001b[1;32m     38\u001b[0m     generation_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m max_new_tokens\n\u001b[0;32m---> 39\u001b[0m     summary \u001b[38;5;241m=\u001b[39m ppo_trainer\u001b[38;5;241m.\u001b[39mgenerate(prompt_tensor, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_kwargs)\n\u001b[1;32m     41\u001b[0m     summary_tensors\u001b[38;5;241m.\u001b[39mappend(summary\u001b[38;5;241m.\u001b[39msqueeze()[\u001b[38;5;241m-\u001b[39mmax_new_tokens:])\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# This needs to be called \"response\".\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llm-diss/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:497\u001b[0m, in \u001b[0;36mPPOTrainer.generate\u001b[0;34m(self, query_tensor, length_sampler, batch_size, return_prompt, generate_ref_response, **generation_kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m     generation_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m length_sampler()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m unwrap_model_for_generation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator) \u001b[38;5;28;01mas\u001b[39;00m unwrapped_model:\n\u001b[0;32m--> 497\u001b[0m     response \u001b[38;5;241m=\u001b[39m unwrapped_model\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m=\u001b[39mquery_tensor\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_kwargs)\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generate_ref_response:\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m unwrap_model_for_generation(\n\u001b[1;32m    501\u001b[0m         ref_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator, is_peft_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_peft_model\n\u001b[1;32m    502\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m unwrapped_model:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llm-diss/lib/python3.11/site-packages/trl/models/modeling_value_head.py:438\u001b[0m, in \u001b[0;36mAutoModelForSeq2SeqLMWithValueHead.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    435\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03m    We call `generate` on the wrapped model.\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrained_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llm-diss/lib/python3.11/site-packages/peft/peft_model.py:1441\u001b[0m, in \u001b[0;36mPeftModelForSeq2SeqLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1440\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1441\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llm-diss/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llm-diss/lib/python3.11/site-packages/transformers/generation/utils.py:1622\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1614\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1615\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1616\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1617\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1618\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1619\u001b[0m     )\n\u001b[1;32m   1621\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1622\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[1;32m   1623\u001b[0m         input_ids,\n\u001b[1;32m   1624\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   1625\u001b[0m         logits_warper\u001b[38;5;241m=\u001b[39mlogits_warper,\n\u001b[1;32m   1626\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   1627\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[1;32m   1628\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[1;32m   1629\u001b[0m         output_logits\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_logits,\n\u001b[1;32m   1630\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[1;32m   1631\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   1632\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[1;32m   1633\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1634\u001b[0m     )\n\u001b[1;32m   1636\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1637\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1638\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1639\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1640\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1646\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llm-diss/lib/python3.11/site-packages/transformers/generation/utils.py:2804\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2801\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m   2803\u001b[0m \u001b[38;5;66;03m# pre-process distribution\u001b[39;00m\n\u001b[0;32m-> 2804\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m logits_processor(input_ids, next_token_logits)\n\u001b[1;32m   2805\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m logits_warper(input_ids, next_token_scores)\n\u001b[1;32m   2807\u001b[0m \u001b[38;5;66;03m# Store scores, attentions and hidden_states when required\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llm-diss/lib/python3.11/site-packages/transformers/generation/logits_process.py:98\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m         scores \u001b[38;5;241m=\u001b[39m processor(input_ids, scores, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m         scores \u001b[38;5;241m=\u001b[39m processor(input_ids, scores)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llm-diss/lib/python3.11/site-packages/transformers/generation/logits_process.py:156\u001b[0m, in \u001b[0;36mMinLengthLogitsProcessor.__call__\u001b[0;34m(self, input_ids, scores)\u001b[0m\n\u001b[1;32m    154\u001b[0m vocab_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], device\u001b[38;5;241m=\u001b[39mscores\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    155\u001b[0m eos_token_id \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meos_token_id, device\u001b[38;5;241m=\u001b[39mscores\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 156\u001b[0m eos_token_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39misin(vocab_tensor, eos_token_id)\n\u001b[1;32m    157\u001b[0m scores_processed \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_length:\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::isin.Tensor_Tensor_out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "output_min_length = 100\n",
    "output_max_length = 3_500\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "preferred_summary_index = 0\n",
    "\n",
    "generation_kwargs = {\"min_length\": 5, \"top_k\": 0.0, \"top_p\": 1.0, \"do_sample\": True}\n",
    "\n",
    "reward_kwargs = {\n",
    "    \"top_k\": None,  # Return all scores.\n",
    "    \"function_to_apply\": \"none\",  # Raw logits without softmax.\n",
    "    \"batch_size\": RL_TRAIN_BATCH_SIZE,\n",
    "}\n",
    "\n",
    "max_ppo_steps = 5\n",
    "\n",
    "\n",
    "for step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    # print(step, batch)\n",
    "    # Break when you reach max_steps.\n",
    "    if step >= max_ppo_steps:\n",
    "        break\n",
    "\n",
    "    prompt_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    # Get response from FLAN-T5/PEFT LLM.\n",
    "    summary_tensors = []\n",
    "\n",
    "    for prompt_tensor in prompt_tensors:\n",
    "        max_new_tokens = output_length_sampler()\n",
    "\n",
    "        generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n",
    "        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n",
    "\n",
    "        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n",
    "\n",
    "    # This needs to be called \"response\".\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in summary_tensors]\n",
    "\n",
    "    # Compute reward outputs.\n",
    "    query_response_pairs = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    rewards = sentiment_pipe(query_response_pairs, **reward_kwargs)\n",
    "\n",
    "    # You use the `nothate` item because this is the score for the positive `nothate` class.\n",
    "    reward_tensors = [\n",
    "        torch.tensor(reward[preferred_summary_index][\"score\"]) for reward in rewards\n",
    "    ]\n",
    "\n",
    "    # Run PPO step.\n",
    "    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n",
    "    ppo_trainer.log_stats(stats, batch, reward_tensors)\n",
    "\n",
    "    print(f'objective/kl: {stats[\"objective/kl\"]}')\n",
    "    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n",
    "    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')\n",
    "    print('-'.join('' for x in range(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b936573c-40c9-4aa4-aa3a-4ecdc9970131",
   "metadata": {},
   "source": [
    "### _Evaluate RL model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3232e2e6-7acf-4745-8fa7-27370ecd98ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EVAL_SAMPLES = int(len(tokenized_datasets['test']) * 0.15)\n",
    "\n",
    "start = time.time()\n",
    "peft_checkpoint_generation = quantitative_comparison(\n",
    "    ppo_model,\n",
    "    dataset,\n",
    "    tokenizer,\n",
    "    n_samples_to_evaluate=N_EVAL_SAMPLES,\n",
    "    batch_size=2,\n",
    "    device=DEVICE,\n",
    ")\n",
    "baseline_model_generation = quantitative_comparison(\n",
    "    sft_model,\n",
    "    dataset,\n",
    "    tokenizer,\n",
    "    n_samples_to_evaluate=N_EVAL_SAMPLES,\n",
    "    batch_size=2,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "duration = end - start\n",
    "print(\n",
    "    f\"Evaluating N={N_EVAL_SAMPLES} samples took {round(duration, 2)} seconds to execute.\"\n",
    ")\n",
    "\n",
    "human_baseline_answer = dataset[\"test\"][0:N_EVAL_SAMPLES][\"summary\"]\n",
    "\n",
    "zipped_summaries = list(\n",
    "    zip(human_baseline_answer, peft_checkpoint_generation, baseline_model_generation)\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    zipped_summaries,\n",
    "    columns=[\n",
    "        \"human_baseline_answer\",\n",
    "        \"peft_checkpoint_generation\",\n",
    "        \"baseline_model_generation\",\n",
    "    ],\n",
    ")\n",
    "df.head()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc74442b-4c6f-416d-926e-1065d3d09055",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=baseline_model_generation,\n",
    "    references=human_baseline_answer[0 : len(baseline_model_generation)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_checkpoint_generation,\n",
    "    references=human_baseline_answer[0 : len(peft_checkpoint_generation)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "original_model_results = round_dictionary_values(original_model_results)\n",
    "# instruct_model_results = round_dictionary_values(instruct_model_results)\n",
    "peft_model_results = round_dictionary_values(peft_model_results)\n",
    "print(\"SFT MODEL:\")\n",
    "print(original_model_results)\n",
    "# print('INSTRUCT MODEL:')\n",
    "# print(instruct_model_results)\n",
    "print(\"PEFT MODEL:\")\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec52cc9-3ce9-40b9-9d40-e46d458880d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(METRICS_PATH):\n",
    "    os.makedirs(METRICS_PATH)\n",
    "\n",
    "data_path = f'{METRICS_PATH}/rl-results.json'\n",
    "\n",
    "results_dict = {'sft-model': original_model_results, 'rl-model': peft_model_results}\n",
    "with open(data_path, 'w') as file:\n",
    "    json.dump(results_dict, file)\n",
    "print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n",
    "\n",
    "improvement = np.array(list(peft_model_results.values())) - np.array(\n",
    "    list(original_model_results.values())\n",
    ")\n",
    "for key, value in zip(peft_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e22f46a-2420-4993-8bd8-8d2be78c86f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add one more eval, for higher rated summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca6f26e-9f26-40d6-81d8-a607004b6c5a",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "empire",
   "language": "python",
   "name": "empire"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
