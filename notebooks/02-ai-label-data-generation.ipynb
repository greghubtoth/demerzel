{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc48c385-d263-409f-a846-ef4b36247b4f",
   "metadata": {},
   "source": [
    "# AI labelled data generation and Reward Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0587c4a1-e544-43fc-bde3-9ac9433c4a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import uuid\n",
    "import json\n",
    "import os\n",
    "\n",
    "from nyx.data_generation import Controller\n",
    "from nyx.data_generation.settings import BASELINE_LEE_ET_AL\n",
    "from nyx.data_loaders import HumanEvaluatedDataLoader\n",
    "from nyx.constants import METRICS_PATH, COMMON_OUTPUT_PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f18a006-413d-4f08-8371-687a08b704ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "# PRECISION = torch.float32\n",
    "PRECISION_NAME = 'float16'\n",
    "DEVICE = \"mps\"\n",
    "LABELLER_MODEL = \"bigscience/mt0-small\"  # \"google/flan-t5-small\"\n",
    "# \"stabilityai/stablelm-2-zephyr-1_6b\"\n",
    "# \"microsoft/phi-1_5\"\n",
    "# \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "# \"google/flan-t5-large\"\n",
    "# \"google/flan-t5-xl\"\n",
    "# \"bigscience/mt0-small\"\n",
    "# \"bigscience/mt0-large\"\n",
    "# \"bigscience/mt0-xl\"\n",
    "GEMMA_PATH = \"/Users/owner/Documents/AI_MSc/13.Dissertation/experiments/labelling-model/gemma-2b-it/\"\n",
    "RUN_ID = \"test\"  #'316a2787976e4e848ea635422e2ef684' # uuid.uuid4().hex\n",
    "TESTING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99cf8bc1-2af8-4dd7-88f7-59cf3e25fea9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8b8ccbe5144845850de238139efebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b95d136a02042eb91dd305ab244eb91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                   | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities being calculated.\n",
      "summary_predictions: [[0.7885831594467163, 0.729519784450531], [0.2114168107509613, 0.270480215549469]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████                                                     | 1/2 [12:41<12:41, 761.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmaxed_log_probabs: [[0.7880859375, 0.7294921875], [0.2113037109375, 0.2705078125]]\n",
      "Probabilities being calculated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [13:14<00:00, 397.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary_predictions: [[0.7885831594467163, 0.729519784450531, 0.7793218493461609, 0.709019124507904], [0.2114168107509613, 0.270480215549469, 0.2206781655550003, 0.29098084568977356]]\n",
      "softmaxed_log_probabs: [[0.779296875, 0.708984375], [0.220703125, 0.291015625]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                   | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities being calculated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████▌                                                     | 1/2 [00:21<00:21, 21.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary_predictions: [[0.6706082820892334, 0.8267117738723755], [0.3293917179107666, 0.17328819632530212]]\n",
      "softmaxed_log_probabs: [[0.67041015625, 0.82666015625], [0.329345703125, 0.1732177734375]]\n",
      "Probabilities being calculated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:42<00:00, 21.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary_predictions: [[0.6706082820892334, 0.8267117738723755, 0.6749081611633301, 0.7341195344924927], [0.3293917179107666, 0.17328819632530212, 0.3250918686389923, 0.2658804655075073]]\n",
      "softmaxed_log_probabs: [[0.6748046875, 0.73388671875], [0.3251953125, 0.265869140625]]\n",
      "Labelling all data twice took 837.15 seconds to execute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f86815875eb4f3bbc954348331a4b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved data to:\n",
      "./experiments/test/data/labelled-train-data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3623b8857824c4f9116a44e4cb75ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt driven, instruct model generated feedback labels are in agreement with the annotator provided labels: 75.0% of the times.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         3\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.75         4\n",
      "   macro avg       0.38      0.50      0.43         4\n",
      "weighted avg       0.56      0.75      0.64         4\n",
      "\n",
      "tp: 0, fp: 0\n",
      "fn: 1, tn: 3\n",
      "MCC is 0.0.\n",
      "Finished generated data evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gtoth/miniconda3/envs/dissertation/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/gtoth/miniconda3/envs/dissertation/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/gtoth/miniconda3/envs/dissertation/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/gtoth/miniconda3/envs/dissertation/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/gtoth/miniconda3/envs/dissertation/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/gtoth/miniconda3/envs/dissertation/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'llm_model_name': LABELLER_MODEL,  # LABELLER_MODEL, # GEMMA_PATH, # 70B param model\n",
    "    'precision_name': PRECISION_NAME,\n",
    "    'device': DEVICE,\n",
    "    # 'dataset': data,\n",
    "    'run_id': RUN_ID,\n",
    "    'max_new_tokens': 512,\n",
    "}\n",
    "\n",
    "\n",
    "data_generator = Controller(\n",
    "    labelling_method=f'{BASELINE_LEE_ET_AL}_single_gpu',  # BASELINE_LEE_ET_AL, # Toth et al, (Ablation)\n",
    "    labelling_config=config,\n",
    "    data_loader=HumanEvaluatedDataLoader,\n",
    ")\n",
    "\n",
    "if TESTING is True:\n",
    "    indices = random.sample(range(0, 92859), 4)\n",
    "    # print(indices)\n",
    "    data_generator.data_to_label[\"train\"] = data_generator.data_to_label[\n",
    "        \"train\"\n",
    "    ].select(indices)\n",
    "    data_generator.data_to_label[\"validation\"] = data_generator.data_to_label[\n",
    "        \"validation\"\n",
    "    ].select(range(50))\n",
    "\n",
    "\n",
    "# print(data_generator.data_to_label)\n",
    "data_generator.label_data()\n",
    "data_generator.report_on_performance()\n",
    "# (data_generator.data_to_label['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20e5814eb404a0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List\n",
    "# from datasets import DatasetDict\n",
    "# from pprint import pprint\n",
    "# def dataset_dict_to_langchain_batch_consumable(data: DatasetDict,\n",
    "#                                                requested_cols: List[str],\n",
    "#                                                data_split: str = 'train', ) -> List[dict]:\n",
    "#     requested_data = data[data_split]\n",
    "#     data_for_langchain = []\n",
    "#     for values in zip(*[requested_data[col] for col in requested_cols]):\n",
    "#         # print(values)\n",
    "#         row_value = {col: values[index] for index, col in enumerate(requested_cols)}\n",
    "#         data_for_langchain.append(row_value)\n",
    "\n",
    "#     return data_for_langchain\n",
    "# pprint(dataset_dict_to_langchain_batch_consumable(data_generator.data_to_label, ['post', 'candidate_summary_1', 'candidate_summary_2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c65e01f7f749ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMON_OUTPUT_PATHS = COMMON_OUTPUT_PATHS.format(RUN_ID=RUN_ID)\n",
    "METRICS_PATH = METRICS_PATH.format(COMMON_OUTPUT_PATHS=COMMON_OUTPUT_PATHS)\n",
    "\n",
    "if not os.path.exists(METRICS_PATH):\n",
    "    os.makedirs(METRICS_PATH)\n",
    "\n",
    "data_path = f'{METRICS_PATH}/data-generation-info.json'\n",
    "\n",
    "results_dict = {\n",
    "    'run_id': RUN_ID,\n",
    "    'labeller_model': LABELLER_MODEL,\n",
    "    'precision': PRECISION_NAME,\n",
    "}\n",
    "with open(data_path, 'w') as file:\n",
    "    json.dump(results_dict, file)\n",
    "\n",
    "print('Data generation done and the configuration info is saved.')\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae089c78-408b-47fd-b0c9-003e2e89f1ba",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-diss",
   "language": "python",
   "name": "llm-diss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
